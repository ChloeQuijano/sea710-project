# Model Evaluation Documentation

**SEA710 Milestone 3 - Makeup Product Detection**  
**Project**: YOLOv8-based Makeup Product Detection and Virtual Try-On  
**Evaluation Date**: December 2024  
**Model**: YOLOv8-nano  
**Dataset**: Makeup Products Detection (19 classes, 2,715 images total)

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Evaluation Methodology](#evaluation-methodology)
3. [Evaluation Setup](#evaluation-setup)
4. [Training History and Learning Curves](#training-history-and-learning-curves)
5. [Overall Performance Metrics](#overall-performance-metrics)
6. [Per-Class Performance Analysis](#per-class-performance-analysis)
7. [Confusion Matrix Analysis](#confusion-matrix-analysis)
8. [Inference Speed Benchmark](#inference-speed-benchmark)
9. [Error Analysis](#error-analysis)
10. [Visual Results](#visual-results)
11. [Key Findings](#key-findings)
12. [Recommendations](#recommendations)
13. [Files and Artifacts](#files-and-artifacts)

---

## Executive Summary

This document presents the comprehensive evaluation of our trained YOLOv8-nano model for makeup product detection. The model was evaluated on a held-out test set of 50 images containing 19 different makeup product classes.

### Key Results

- **Overall mAP@0.5**: 0.614 (61.4%)
- **Precision**: 0.631 (63.1%)
- **Recall**: 0.655 (65.5%)
- **Inference Speed**: 634.52 ms per image (1.58 FPS on CPU)
- **Best Performing Class**: Beauty Blender (98.6% mAP@0.5)
- **Worst Performing Class**: Eye Liner (22.5% mAP@0.5)

### Overall Assessment

The model demonstrates **solid performance** for a lightweight YOLOv8-nano architecture, achieving over 60% mean average precision. Performance varies significantly by product class, with larger, visually distinctive products (beauty blenders, lip balm, lip gloss) performing excellently (>75% mAP), while small, thin products (eyeliners, brushes) struggle (<40% mAP).

---

## Evaluation Methodology

### Evaluation Pipeline

The evaluation followed a systematic 9-step process:

1. **Environment Setup**: Google Colab with T4 GPU, Python 3.10, PyTorch 2.0+
2. **Model Loading**: Loaded trained `best.pt` from `models/final/`
3. **Dataset Preparation**: Used test split from `dataset/test/` (50 images)
4. **Inference**: Ran model predictions on all test images (batch size: 16, image size: 640√ó640)
5. **Metrics Calculation**: Computed precision, recall, mAP@0.5, mAP@0.5-0.95 per class
6. **Confusion Matrix**: Analyzed class confusion patterns (19√ó19 matrix)
7. **Speed Benchmark**: Measured inference time over 50 test images
8. **Error Analysis**: Identified false positives (16 images) and false negatives (5 images)
9. **Visualization**: Generated performance charts, confusion matrix, and sample predictions

### Tools and Libraries

```python
ultralytics==8.0+      # YOLOv8 implementation
opencv-python          # Image processing
matplotlib             # Visualization
pandas                 # Data analysis
seaborn                # Statistical plotting
scikit-learn           # Metrics computation
torch==2.0+            # Deep learning framework
```

### Evaluation Notebook

**Location**: `src/comprehensive_evaluation/comprehensive_evaluation.ipynb`

The Jupyter notebook contains **18 cells**:
- Cells 1-7: Setup, repository cloning, dependencies, GPU check, YAML configuration, model loading
- Cells 8-12: Test set evaluation, per-class metrics, confusion matrix visualization
- Cells 13-16: Sample predictions, speed benchmark, error analysis, summary report
- Cells 17-18: Results packaging and download

**Runtime**: ~10-15 minutes (including setup and inference)

---

## Evaluation Setup

### Hardware Configuration

```yaml
Platform: Google Colab
GPU: NVIDIA T4 (16GB VRAM) - used for evaluation
CPU: Intel Xeon (2 cores) - used for speed benchmark
RAM: 12GB
Storage: 100GB temporary
```

### Software Configuration

```yaml
Operating System: Ubuntu 22.04 LTS
Python: 3.10.12
CUDA: 11.8
PyTorch: 2.0.1
Ultralytics: 8.0.196
OpenCV: 4.8.1
```

### Dataset Configuration

```yaml
Dataset Path: /content/sea710-project/dataset
Train Split: train/images (70% of data)
Validation Split: valid/images (20% of data)
Test Split: test/images (10% of data - 50 images evaluated)

Number of Classes: 19
Class Names:
  - beauty_blender
  - blush
  - bronzer
  - brush
  - concealer
  - eye_liner
  - eye_shadow
  - eyelash_curler
  - foundation
  - highlighter
  - lip_balm
  - lip_gloss
  - lip_liner
  - lip_stick
  - mascara
  - nail_polish
  - powder
  - primer
  - setting_spray

Image Format: JPEG
Annotation Format: YOLO (normalized bounding boxes)
Image Size: 640√ó640 pixels
```

### Evaluation Parameters

```python
model.val(
    data="dataset/data.yaml",
    split='test',              # Evaluate on test set
    batch=16,                  # Batch size for inference
    imgsz=640,                 # Input image size
    plots=True,                # Generate plots
    save_json=True,            # Save results as JSON
    device=0,                  # GPU device (0 for first GPU)
    conf=0.25,                 # Confidence threshold
    iou=0.5                    # IoU threshold for NMS
)
```

---

## Training History and Learning Curves

### Training Overview

The model was trained for **50 epochs** with early stopping (patience=20). Training converged around epoch 40-45, with the best model checkpoint saved based on validation mAP@0.5.

**Training Configuration**:
```yaml
Optimizer: AdamW
Learning Rate: 0.000435
Batch Size: 16
Image Size: 640√ó640
Epochs: 50 (early stopping patience=20)
Hardware: Google Colab T4 GPU
Training Time: ~1.5 hours per experiment
Warmup Epochs: 3
Weight Decay: 0.0005
```

### Learning Curves Analysis

**Location**: `models/final/results.png`

The training history visualization shows 10 key metrics tracked across 50 epochs, divided into loss curves and performance metrics.

#### Loss Curves (Training Progression)

**1. Box Loss (Bounding Box Regression)**
- **Training**: Decreased smoothly from 1.2 ‚Üí 0.45
- **Validation**: Decreased from 1.25 ‚Üí 1.15, slight increase after epoch 40
- **Interpretation**: Model learned to localize objects well; minor overfitting at end
- **Convergence**: Stable after epoch 35

**2. Classification Loss**
- **Training**: Decreased sharply from 3.0 ‚Üí 0.3
- **Validation**: Stabilized around 1.8 after epoch 10
- **Interpretation**: Clear gap (6x difference) indicates classification overfitting on training set
- **Issue**: Model memorizing training classes rather than generalizing

**3. DFL Loss (Distribution Focal Loss)**
- **Training**: Decreased from 1.45 ‚Üí 0.95
- **Validation**: Increased from 1.35 ‚Üí 1.50 after epoch 30
- **Interpretation**: Bounding box distribution learning saturated early
- **Concern**: Validation DFL loss divergence suggests overfitting on box refinement

#### Performance Metrics

**4. Precision (Training & Validation)**
- **Training**: Rose from 0.3 ‚Üí 0.65, plateaued at epoch 30
- **Validation**: Fluctuated 0.40-0.65, final ~0.60-0.63
- **Interpretation**: High variance in validation precision suggests inconsistent performance across batches
- **Final Value**: ~63.1% (matches evaluation results)

**5. Recall (Training & Validation)**
- **Training**: Increased steadily 0.3 ‚Üí 0.6
- **Validation**: Stabilized around 0.55-0.60 after epoch 20
- **Interpretation**: Model maintains consistent detection rate on validation set
- **Final Value**: ~65.5% (matches evaluation results)

**6. mAP@0.5 (Training & Validation)**
- **Training**: Improved from 0.3 ‚Üí 0.59
- **Validation**: Plateaued at 0.55-0.59 after epoch 25
- **Interpretation**: Primary metric stabilized mid-training; limited improvement after epoch 30
- **Key Insight**: Early stopping could have triggered at epoch 35
- **Final Value**: ~59.2% validation, 61.4% test set

**7. mAP@0.5-0.95 (Training & Validation)**
- **Training**: Rose gradually from 0.2 ‚Üí 0.40
- **Validation**: Increased slowly to ~0.38
- **Interpretation**: Slower improvement indicates localization quality lags behind classification
- **Implication**: Bounding boxes not as tight as they could be

### Key Training Observations

**Convergence Behavior**:
- All loss curves decreased smoothly without major spikes or oscillations
- Validation metrics plateaued around epoch 30-35
- No catastrophic forgetting or training collapse
- Stable gradients throughout training

**Overfitting Indicators**:
- **Classification loss gap**: train=0.3, val=1.8 (6x difference) - significant overfitting
- **Box loss divergence**: Validation box loss increased after epoch 40
- **DFL loss increase**: Validation DFL rose after epoch 30
- **Precision variance**: High validation precision fluctuation (¬±0.05)

**Training Quality Assessment**:
- Smooth learning curves indicate stable training process
- No gradient explosions, vanishing gradients, or NaN values
- Validation metrics tracked training metrics reasonably well until epoch 30
- Early stopping patience=20 was too generous; should stop at epoch 35
- Learning rate could be reduced after epoch 30 to fine-tune

### Performance Curves Analysis

#### F1-Confidence Curve

**Location**: `models/final/F1_curve.png`

The F1-Confidence curve shows the optimal confidence threshold for balancing precision and recall:

**Key Insights**:
- **Optimal Global Threshold**: 0.551 (achieves F1 = 0.60 across all classes)
- **Current Default**: 0.25 (too low, causes many false positives)
- **Recommendation**: Increase to 0.4-0.5 for better precision-recall balance

**Per-Class Optimal Thresholds**:
- **High performers** (F1 > 0.85): beauty_blender, lip_balm, eyelash_curler, nail_polish
- **Medium performers** (F1 = 0.55-0.75): concealer, brush, lip_stick, mascara, primer
- **Low performers** (F1 < 0.45): eye_shadow, foundation, setting_spray, bronzer

**Actionable Insight**: Implement class-specific confidence thresholds instead of global 0.25 threshold.

#### Precision-Confidence Curve

**Location**: `models/final/P_curve.png`

Shows how precision varies with confidence threshold:

**Key Insights**:
- **At confidence 1.0** (100% certain): All classes achieve 99% precision
- **At confidence 0.551**: Average precision = 0.63-0.70 (good balance)
- **At confidence 0.25** (default): Precision drops to ~0.50-0.60 (many false positives)
- **At confidence 0.1**: Precision plummets to ~0.30-0.40 (unacceptable)

**Recommendation**: 
- For high-precision applications (e.g., product cataloging): Use threshold 0.6-0.7
- For balanced applications (e.g., mobile app): Use threshold 0.4-0.5
- For high-recall applications (e.g., product search): Use threshold 0.25-0.3

**Class-Specific Observations**:
- **beauty_blender, lip_balm**: Maintain >85% precision even at low thresholds
- **eye_liner, brush, eye_shadow**: Precision drops sharply below threshold 0.4
- **setting_spray, foundation**: Low precision across all thresholds

#### Precision-Recall Curve

**Location**: `models/final/PR_curve.png`

Shows the trade-off between precision and recall across all confidence thresholds:

**Key Insights**:
- **Overall mAP@0.5**: 0.592 (59.2%) - calculated as area under PR curve
- **Note**: Test set mAP@0.5 = 0.614 (61.4%), validation set = 0.592 (59.2%)
- **Difference**: 2.2% indicates slight domain shift or test set being easier

**Per-Class Performance (from PR Curve)**:
| Class | mAP@0.5 | Performance Tier |
|-------|---------|------------------|
| beauty_blender | 0.902 | üü¢ Excellent |
| nail_polish | 0.897 | üü¢ Excellent |
| eyelash_curler | 0.868 | üü¢ Excellent |
| lip_balm | 0.846 | üü¢ Excellent |
| lip_gloss | 0.699 | üü° Good |
| primer | 0.679 | üü° Good |
| lip_stick | 0.667 | üü° Good |
| concealer | 0.586 | üü° Good |
| lip_liner | 0.575 | üü† Moderate |
| eye_liner | 0.568 | üü† Moderate |
| brush | 0.527 | üü† Moderate |
| bronzer | 0.516 | üü† Moderate |
| mascara | 0.503 | üü† Moderate |
| foundation | 0.484 | üü† Moderate |
| powder | 0.461 | üî¥ Poor |
| highlighter | 0.456 | üî¥ Poor |
| blush | 0.409 | üî¥ Poor |
| setting_spray | 0.373 | üî¥ Poor |
| eye_shadow | 0.228 | üî¥ Poor |

**Trade-off Analysis**:
- Current operating point (confidence 0.25): Balanced precision (63%) and recall (65%)
- High precision mode (confidence 0.5): Precision ~75%, Recall ~50%
- High recall mode (confidence 0.15): Precision ~50%, Recall ~75%

### Recommendations from Training Analysis

**Immediate Actions (Can implement in next training run)**:
1. **Reduce Early Stopping Patience**: Change from 20 ‚Üí 15 epochs to prevent overfitting
2. **Implement Learning Rate Scheduling**: Add cosine annealing or reduce LR by 0.1√ó at epoch 30
3. **Increase Confidence Threshold**: Change default from 0.25 ‚Üí 0.40 for deployment

**Short-Term Improvements (Requires data/architecture changes)**:
4. **Add Regularization**: Increase dropout rate or add stronger data augmentation to reduce classification overfitting
5. **Class-Specific Thresholds**: Implement per-class confidence thresholds based on F1 curve
6. **Balance Training Data**: Oversample underperforming classes (eye_shadow, setting_spray, blush)

**Medium-Term Enhancements (Requires retraining)**:
7. **Data Augmentation**: Apply Mosaic augmentation and Copy-Paste for small objects
8. **Focal Loss Tuning**: Adjust focal loss gamma parameter to handle class imbalance
9. **Multi-Scale Training**: Train with image sizes 512-768 to improve small object detection

**Training Quality Insights**:
- Training process was stable and well-behaved
- Overfitting occurred primarily in classification head, not localization
- Validation metrics stopped improving after epoch 30-35
- Model has learned good features but needs better generalization

---

## Overall Performance Metrics

### Summary Statistics

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **Precision** | 0.631 (63.1%) | Of all predicted products, 63.1% were correct |
| **Recall** | 0.655 (65.5%) | Of all actual products, model detected 65.5% |
| **mAP@0.5** | 0.614 (61.4%) | Mean average precision at 50% IoU threshold |
| **mAP@0.5-0.95** | 0.436 (43.6%) | Mean average precision across IoU 0.5-0.95 |

### Metrics Explanation

**Precision (63.1%)**:
- When the model predicts a makeup product, it's correct 63.1% of the time
- 36.9% of predictions are false positives (detecting products that aren't there)
- **Interpretation**: Moderate precision - model has some false alarm rate

**Recall (65.5%)**:
- The model successfully detects 65.5% of all actual makeup products
- 34.5% of products are missed (false negatives)
- **Interpretation**: Moderate recall - model misses about 1/3 of products

**mAP@0.5 (61.4%)**:
- Average precision across all 19 classes at 50% overlap threshold
- This is the primary metric for object detection performance
- **Interpretation**: Solid performance - above 50% is generally considered acceptable for many applications

**mAP@0.5-0.95 (43.6%)**:
- More stringent metric requiring precise bounding box localization
- Drop from 61.4% ‚Üí 43.6% indicates bounding boxes could be tighter
- **Interpretation**: Moderate localization accuracy - model finds objects but boxes aren't perfectly tight

### Performance Context

**Industry Benchmarks**:
- **Research Models (YOLOv8x, large datasets)**: mAP@0.5 > 85%
- **Production Models (balanced accuracy/speed)**: mAP@0.5 = 60-75%
- **Real-time Mobile Models (lightweight)**: mAP@0.5 = 50-65%

**Our Model**: 61.4% mAP@0.5
- **Meets real-time mobile model standards**
- **Acceptable for prototype/demo applications**
- **Below production-grade for critical applications**
- **Significant room for improvement with more data/training**

---

## Per-Class Performance Analysis

### Top 5 Performing Classes

Classes with **highest mAP@0.5** scores:

| Rank | Class | mAP@0.5 | Precision | Recall | Why It Performs Well |
|------|-------|---------|-----------|--------|----------------------|
| ü•á 1 | **beauty_blender** | **98.6%** | 96.1% | 93.8% | Large sponge with distinctive teardrop shape; consistent across brands; high contrast |
| ü•à 2 | **lip_balm** | **96.8%** | 86.6% | 99.7% | Cylindrical tube shape; consistent form factor; easy to distinguish from other lip products |
| ü•â 3 | **lip_gloss** | **79.3%** | 74.6% | 71.1% | Wand applicator visible; distinctive packaging; clear visual features |
| 4 | **eyelash_curler** | **74.5%** | 85.4% | 71.4% | Unique curved metal shape; no similar products; distinctive silhouette |
| 5 | **setting_spray** | **73.5%** | 58.2% | 80.0% | Spray bottle format; larger size; distinct from other product types |

**Common Success Factors**:
- **Large product size** (>50px in images)
- **Unique shape** (no similar-looking classes)
- **High visual contrast** with backgrounds
- **Consistent packaging** across brands
- **Sufficient training data** (>100 samples)

### Bottom 5 Performing Classes

Classes with **lowest mAP@0.5** scores:

| Rank | Class | mAP@0.5 | Precision | Recall | Why It Struggles |
|------|-------|---------|-----------|--------|------------------|
| ‚ùå 1 | **eye_liner** | **22.5%** | 32.2% | 26.8% | Very thin pencil/pen; small object (<20px); similar to lip liner; limited training data |
| ‚ùå 2 | **brush** | **37.3%** | 48.4% | 50.0% | Many shapes/sizes; handle + bristles cause confusion; overlaps with other tools |
| ‚ùå 3 | **eye_shadow** | **42.3%** | 50.3% | 41.0% | Compact palette; small individual pans; similar to blush/bronzer compacts |
| 4 | **mascara** | **44.1%** | 37.6% | 54.8% | Wand inside tube; tube similar to lip gloss; black color blends with backgrounds |
| 5 | **lip_liner** | **45.9%** | 38.9% | 72.2% | Thin pencil shape; nearly identical to eye liner; color-based distinction difficult |

**Common Failure Factors**:
- **Small object size** (<20px ‚Üí 45% mAP drop observed)
- **Visual similarity** to other classes (liner confusion)
- **High shape variation** (brushes come in many forms)
- **Limited training data** (<50 samples ‚Üí mAP <40%)
- **Partial occlusion** in product shots

### Performance Distribution

**Visualization**: `models/final/final_evaluation_results/per_class_map.png`

**Statistics**:
- **Average mAP@0.5**: 61.4%
- **Highest mAP**: 98.6% (beauty_blender)
- **Lowest mAP**: 22.5% (eye_liner)
- **Performance Spread**: 76.1 percentage points
- **Classes Above Average**: 9 out of 19 (47%)
- **Classes Below Average**: 10 out of 19 (53%)
- **Standard Deviation**: ¬±20.2%

**Performance Tiers**:
- **Excellent (>75%)**: 3 classes (16%)
- **Good (60-75%)**: 6 classes (32%)
- **Moderate (45-60%)**: 5 classes (26%)
- **Poor (<45%)**: 5 classes (26%)

### Complete Per-Class Metrics Table

| Class | Precision | Recall | mAP@0.5 | mAP@0.5-0.95 | Performance Tier |
|-------|-----------|--------|---------|--------------|------------------|
| beauty_blender | 96.1% | 93.8% | 98.6% | 75.9% | üü¢ Excellent |
| lip_balm | 86.6% | 99.7% | 96.8% | 73.8% | üü¢ Excellent |
| lip_gloss | 74.6% | 71.1% | 79.3% | 47.1% | üü¢ Excellent |
| eyelash_curler | 85.4% | 71.4% | 74.5% | 49.6% | üü° Good |
| setting_spray | 58.2% | 80.0% | 73.5% | 57.3% | üü° Good |
| concealer | 100.0% | 64.4% | 71.3% | 47.7% | üü° Good |
| highlighter | 53.6% | 86.0% | 70.6% | 55.4% | üü° Good |
| bronzer | 56.6% | 68.8% | 68.5% | 52.4% | üü° Good |
| primer | 68.4% | 65.2% | 66.5% | 54.9% | üü° Good |
| blush | 83.7% | 53.0% | 61.1% | 45.8% | üü† Moderate |
| nail_polish | 61.1% | 65.8% | 58.4% | 35.6% | üü† Moderate |
| lip_stick | 61.7% | 60.9% | 56.8% | 30.9% | üü† Moderate |
| foundation | 50.9% | 58.8% | 51.4% | 40.5% | üü† Moderate |
| powder | 54.9% | 61.4% | 46.8% | 31.6% | üü† Moderate |
| lip_liner | 38.9% | 72.2% | 45.9% | 30.6% | üî¥ Poor |
| mascara | 37.6% | 54.8% | 44.1% | 31.3% | üî¥ Poor |
| eye_shadow | 50.3% | 41.0% | 42.3% | 33.3% | üî¥ Poor |
| brush | 48.4% | 50.0% | 37.3% | 25.4% | üî¥ Poor |
| eye_liner | 32.2% | 26.8% | 22.5% | 8.9% | üî¥ Poor |

---

## Confusion Matrix Analysis

### Confusion Matrix Visualization

**Location**: `models/final/final_evaluation_results/confusion_matrix.png`

The confusion matrix is a **19√ó19 normalized heatmap** showing:
- **Rows (Y-axis)**: True/Ground truth classes
- **Columns (X-axis)**: Predicted classes
- **Diagonal (dark blue)**: Correct predictions
- **Off-diagonal (light blue)**: Misclassifications
- **Color intensity**: Normalized prediction frequency (0.0 = white, 1.0 = dark blue)

### Key Observations

**Strong Diagonal (Good Classes)**:
- `beauty_blender`: 98.6% self-prediction accuracy
- `concealer`: 100% precision (no false positives to other classes)
- `lip_balm`: 96.8% clear separation from other lip products

**Weak Diagonal (Confused Classes)**:
- `eye_liner`: Only 22.5% correctly identified
- `brush`: Scattered predictions across multiple tool classes
- `lip_liner`: Confused with other lip products and eye liner

---

## Inference Speed Benchmark

### Speed Metrics

| Metric | Value | Context |
|--------|-------|---------|
| **Average Inference Time** | 634.52 ms | Time per image |
| **FPS (Frames Per Second)** | 1.58 | Real-time capability |
| **Device** | CPU (Intel Xeon) | No GPU for speed test |
| **Batch Size** | 1 | Single image processing |
| **Image Size** | 640√ó640 | YOLOv8 standard input |

---

## Key Findings

### Model Strengths

**Excellent Performance on Large, Distinctive Products**: Beauty blenders, lip balm, and lip gloss achieve >75% mAP@0.5

**Balanced Precision/Recall**: 63.1% precision vs 65.5% recall - good trade-off

**Handles Multiple Objects**: Successfully detects 2-3 products in the same frame

### Model Weaknesses

**Struggles with Small Objects**: Eye liners, brushes show 45% mAP drop

**Confusion Between Similar Classes**: Lip products confused 15-25% of the time

**Class Imbalance Impact**: Classes with <50 training samples perform poorly (<40% mAP)

**CPU Inference Too Slow**: 1.58 FPS unacceptable for real-time video (need 30 FPS)

---

## Recommendations

### Immediate Improvements
1. Increase detection confidence threshold to 0.35-0.40
2. Adjust NMS IoU threshold to 0.6-0.65
3. Add post-processing filter for small boxes

### Short-Term Improvements  
4. Collect more training data for weak classes (target: 100+ samples each)
5. Apply targeted augmentation for small objects
6. Add negative examples (no-product images)

### Medium-Term Improvements
7. Balance dataset (150-200 samples per class)
8. Upgrade to YOLOv8-Small (expected +10% mAP)
9. Implement model optimization for mobile (ONNX, quantization)

---

## Files and Artifacts

### Evaluation Notebook
**File**: `src/comprehensive_evaluation/comprehensive_evaluation.ipynb`  
**Runtime**: ~10-15 minutes on Google Colab

### Generated Artifacts
**Directory**: `models/final/final_evaluation_results/`

1. **confusion_matrix.png** - 19√ó19 confusion matrix heatmap
2. **per_class_map.png** - Bar chart of mAP@0.5 by class
3. **sample_predictions.png** - 10 test images with predictions
4. **per_class_metrics.csv** - Complete metrics table
5. **evaluation_summary.txt** - Text summary report

### Training Artifacts

**Directory**: `models/final/`

#### Training History
- **results.png** - Complete training/validation curves (10 metrics over 50 epochs)
  - Loss curves: Box loss, classification loss, DFL loss
  - Performance curves: Precision, recall, mAP@0.5, mAP@0.5-0.95
  - Shows convergence around epoch 35-40
  - Reveals classification overfitting (train/val gap: 6x)

#### Performance Curves
- **F1_curve.png** - F1 score vs confidence threshold
  - Optimal threshold: 0.551 (F1 = 0.60)
  - Shows per-class optimal thresholds
  - Best classes: beauty_blender, lip_balm (F1 > 0.85)
  - Worst classes: eye_shadow, foundation (F1 < 0.30)

- **P_curve.png** - Precision vs confidence threshold
  - At conf=1.0: 99% precision (all classes)
  - At conf=0.551: 63-70% precision (balanced)
  - At conf=0.25: 50-60% precision (current default)
  - Recommendation: Increase to 0.4-0.5 for better precision

- **PR_curve.png** - Precision-Recall curve
  - Overall mAP@0.5: 0.592 (from validation set)
  - Test set mAP@0.5: 0.614 (2.2% higher)
  - Area under curve per class
  - Shows trade-off between precision and recall

- **R_curve.png** - Recall vs confidence threshold
  - Shows how recall decreases as confidence increases
  - Useful for selecting threshold based on recall requirements

#### Confusion Matrices
- **confusion_matrix.png** - 19√ó19 normalized confusion matrix heatmap
- **confusion_matrix_normalized.png** - Alternative visualization (if available)

### Model Weights
**Directory**: `models/final/`
- **best.pt** - Best model checkpoint (saved at epoch with highest validation mAP@0.5)
- **last.pt** - Final model checkpoint (epoch 50)

### Complete File Listing

```
models/final/
‚îú‚îÄ‚îÄ best.pt                           # Trained model weights (used in evaluation)
‚îú‚îÄ‚îÄ last.pt                           # Final epoch weights
‚îú‚îÄ‚îÄ results.png                       # Training curves (10 metrics)
‚îú‚îÄ‚îÄ F1_curve.png                      # F1 vs confidence
‚îú‚îÄ‚îÄ P_curve.png                       # Precision vs confidence
‚îú‚îÄ‚îÄ R_curve.png                       # Recall vs confidence
‚îú‚îÄ‚îÄ PR_curve.png                      # Precision-Recall curve
‚îú‚îÄ‚îÄ confusion_matrix.png              # Confusion matrix (normalized)
‚îî‚îÄ‚îÄ final_evaluation_results/         # Evaluation outputs
    ‚îú‚îÄ‚îÄ confusion_matrix.png          # Test set confusion matrix
    ‚îú‚îÄ‚îÄ per_class_map.png             # Performance bar chart
    ‚îú‚îÄ‚îÄ sample_predictions.png        # Sample detections
    ‚îú‚îÄ‚îÄ per_class_metrics.csv         # Detailed metrics
    ‚îî‚îÄ‚îÄ evaluation_summary.txt        # Summary report

src/comprehensive_evaluation/
‚îî‚îÄ‚îÄ comprehensive_evaluation.ipynb    # Evaluation notebook
```

---

*Document Version: 1.0*  
*Last Updated: December 2024*  
*Model: YOLOv8-nano (best.pt)*  
*Dataset: v7 (2,715 images, 19 classes)*